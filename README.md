````markdown
# ğŸ§  Offline AI Assistant

A **fully offline AI assistant** that lets you upload your own filesâ€”PDFs, CSVs, TXT, etc.â€”and ask intelligent questions about them using a locally running LLM like **LLaMA3** (via [Ollama](https://ollama.com)).

No cloud. No subscriptions. No data leaving your machine.

---

## âœ… Why Use This?

- ğŸ” **Your data stays local** â€“ zero risk of leaks
- ğŸ†“ **Completely free** â€“ no API costs or token limits
- âš¡ **Boot when needed** â€“ start/stop with Docker Compose
- ğŸ“„ **Ask anything about your files** â€“ document Q&A made simple
- ğŸ³ **Runs in Docker** â€“ portable, secure, and isolated

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/offline_ai_assistant.git
cd offline_ai_assistant
````

### 2. Start with Docker

```bash
docker-compose up --build
```

> This launches:
>
> * FastAPI backend
> * Ollama model container (e.g., LLaMA3)

---

## ğŸŒ Access the Web Interface

Visit:

```
http://localhost:8000
```

From there, you can:

* ğŸ“ Upload a file (PDF, CSV, TXT, etc.)
* â“ Ask questions about its contents
* ğŸ§¹ Purge datasets anytime

---

## ğŸ§© System Design Diagram

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         Web UI              â”‚
              â”‚   (chat.html via FastAPI)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚            FastAPI              â”‚
          â”‚            main.py              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚                    â”‚
         â–¼                 â–¼                    â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  train.py  â”‚   â”‚     query.py       â”‚  â”‚ config_loader.py â”‚
 â”‚ (vectorize)â”‚   â”‚ (semantic search + â”‚  â”‚  (settings like  â”‚
 â”‚            â”‚   â”‚   LLM interaction) â”‚  â”‚   token limits)  â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚
      â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ /data folder â”‚     â”‚   Ollama     â”‚
â”‚(dataset storeâ”‚     â”‚(LLM service) â”‚
â”‚  on disk)    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow Diagram

```
                 User
                  â”‚
                  â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Upload a File    â”‚
        â”‚ (via chat.html)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ FastAPI `/upload`  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    train.py        â”‚
        â”‚ - Parse file       â”‚
        â”‚ - Embed with model â”‚
        â”‚ - Store in /data   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Ask a Question     â”‚
        â”‚ (via chat.html)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ FastAPI `/query`   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   query.py         â”‚
        â”‚ - Search vectors   â”‚
        â”‚ - Select context   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Ollama         â”‚
        â”‚  (e.g., LLaMA3)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Return Answer      â”‚
        â”‚ (stream to UI)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§° Low-Level Design

### ğŸ“ Folder Structure

```
offline_ai_assistant/
â”œâ”€â”€ main.py                # FastAPI app with routes
â”œâ”€â”€ train.py               # Embeds uploaded files
â”œâ”€â”€ query.py               # Performs vector search + LLM prompt
â”œâ”€â”€ config_loader.py       # Token config loader
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile             # FastAPI build config
â”œâ”€â”€ docker-compose.yml     # Orchestrates API + Ollama
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ chat.html          # Minimal web UI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ <dataset_name>/    # Vector storage
```

---

### ğŸ”Œ API Endpoints

| Method | Endpoint    | Description             |
| ------ | ----------- | ----------------------- |
| GET    | `/`         | UI home page            |
| POST   | `/upload`   | Train on uploaded file  |
| POST   | `/query`    | Ask a question          |
| POST   | `/purge`    | Remove dataset          |
| GET    | `/datasets` | List available datasets |

---

### ğŸ§  Core Logic

**Embedding Flow** (`train.py`)

```python
def process_file(file, dataset_name):
    chunks = split_file_into_chunks(file)
    embeddings = model.encode(chunks)
    save_as_faiss(embeddings, dataset_name)
```

**Query Flow** (`query.py`)

```python
def search_and_respond(dataset, question):
    context = top_k_similar_chunks(question, dataset)
    prompt = f"{context}\n\nQ: {question}"
    return query_ollama(prompt)
```

**Ollama Streaming**

```python
def query_ollama(prompt):
    yield from requests.post("http://ollama:11434/api/generate", json={
        "model": "llama3",
        "prompt": prompt,
        "stream": True
    }, stream=True).iter_lines()
```

---

## ğŸ“„ Supported File Types

* `.pdf`
* `.txt`
* `.csv`
* `.xlsx`
* `.json`

You can extend file handling in `train.py`.

---

## ğŸ§¹ Remove Dataset

Use the â€œPurge Datasetâ€ button or call the `/purge` endpoint to delete a dataset folder from `/data/`.

---

## ğŸ›  Requirements

* Docker + Docker Compose
* 8GB+ RAM (16GB recommended)
* GPU support optional (Ollama supports CUDA & Apple Metal)

---

## ğŸ“ˆ Scalability Notes

| Area         | Future Ideas                                       |
| ------------ | -------------------------------------------------- |
| UI           | Add streaming, markdown, and chat history          |
| Persistence  | Use SQLite or LiteDB to track session logs         |
| Vector Store | Swap FAISS with Qdrant/Weaviate for production use |
| Auth         | Add basic token auth for multi-user access         |
| Model        | Swap Ollama for LM Studio, llama.cpp, or local API |

---

## ğŸ™Œ Contributing

1. Fork the repo
2. Create your branch
3. Submit a PR

Open to improvements in:

* File parsing
* Model routing
* UI enhancements

---

## ğŸ“œ License

MIT License. See `LICENSE`.

---
